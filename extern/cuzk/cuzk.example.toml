# cuzk daemon configuration
#
# Copy to /data/zk/cuzk.toml or pass via --config flag.
# All fields have sensible defaults — this file only needs to
# contain values you want to override.

[daemon]
# Listen address. TCP or Unix domain socket.
# TCP:   "0.0.0.0:9820"
# UDS:   "unix:///run/curio/cuzk.sock"
listen = "0.0.0.0:9820"

[srs]
# Directory containing Filecoin proof parameter files (.params, .vk).
# Must contain the relevant v28-*.params files for the proof types you run.
# Download with: curio fetch-params 32
param_cache = "/data/zk/params"

# SRS entries to pre-warm at daemon startup.
# This sets the environment so the first proof call per circuit type
# populates GROTH_PARAM_MEMORY_CACHE automatically.
# Values: "porep-32g", "wpost-32g", "winning-32g", "snap-32g"
preload = ["porep-32g"]

[memory]
# Maximum CUDA pinned memory budget for SRS residency.
# Phase 1+: controls how much SRS data is kept in pinned memory.
pinned_budget = "50GiB"

# Maximum working memory for proof operations.
working_memory_budget = "80GiB"

[gpus]
# GPU ordinals to use. Empty = auto-detect all GPUs.
# Example: [0, 1] to use only the first two GPUs.
devices = []

# CPU threads for the GPU-side work (b_g2_msm, preprocessing).
# 0 = auto-detect (all CPUs).
#
# With partition-level pipelining (partition_workers > 0), synthesis workers
# and b_g2_msm compete for DDR5 bandwidth and L3 cache. Reducing the pool
# from all-cores to 32 threads cuts b_g2_msm's bucket RAM from ~1.1 GiB to
# ~192 MB, dramatically reducing L3 cache interference with synthesis.
#
# b_g2_msm slows from ~0.5s to ~1.7s but runs outside the GPU lock (Phase 12
# split API), so this does NOT affect GPU throughput.
#
# Recommended: 32 for machines with partition_workers >= 10.
# Use 0 (auto) only for low-concurrency or single-proof workloads.
gpu_threads = 32

# Number of GPU worker tasks per physical GPU (Phase 8: dual-worker interlock).
#
# With 2 workers, one worker's CPU preprocessing overlaps with the other's
# CUDA kernel execution on the same GPU. Only the CUDA kernel region is
# serialized by a per-GPU mutex — CPU work (preprocessing, b_g2_msm,
# proof assembly, Rust bookkeeping) runs freely in parallel.
#
# This eliminates the 10-200ms GPU idle gaps between partition proves:
#   Worker A: [CPU prep][══ CUDA ══][b_g2+epilogue][CPU prep][══ CUDA ══]
#   Worker B:           [CPU prep──][══ CUDA ══][b_g2+epilogue][CPU prep]
#   GPU:                [████ A ████][████ B ████][████ A ████][████ B ██]
#
#   1 = single worker — use on low-memory systems (pw <= 7) where the GPU
#       is already synthesis-starved. Adds no memory overhead.
#   2 = dual-worker interlock — recommended when pw >= 10 and synthesis can
#       keep the GPU channel saturated. ~6% throughput gain over gw=1 at pw=10.
#   3+ = diminishing returns (CPU ~1.3s < CUDA ~2.1s, so 2 workers suffice)
gpu_workers_per_device = 2

[scheduler]
# Max proofs to batch into a single GPU invocation (same circuit type).
#
# Phase 3: Cross-sector batching for PoRep C2 and SnapDeals.
# When multiple same-type proof requests arrive, the engine accumulates
# them into a batch and processes all sectors in a single synthesis + GPU
# pass. This amortizes fixed GPU costs and improves SM utilization.
#
# Memory impact (PoRep 32G, batch synthesis — all partitions at once):
#   batch=1: ~136 GiB intermediate (10 circuits)    → needs 256+ GiB RAM
#   batch=2: ~272 GiB intermediate (20 circuits)    → needs 384+ GiB RAM
#   batch=3: ~408 GiB intermediate (30 circuits)    → needs 512+ GiB RAM
#
# Throughput improvement (approximate, PoRep C2):
#   batch=1: baseline (Phase 2)
#   batch=2: ~1.5x throughput (amortize GPU fixed costs)
#   batch=3: ~2.0x throughput
#
# Set to 1 to disable batching (Phase 2 behavior).
# Set to 2-3 on large-memory machines for higher throughput.
# WinningPoSt and WindowPoSt bypass batching regardless of this setting.
max_batch_size = 1

# Max time (ms) to wait for batch to fill before flushing.
# When a batchable proof request arrives and the batch is not yet full,
# the engine waits up to this duration for more same-type requests.
# Set to 0 to flush immediately (lower latency, less batching).
# Set to 30000-60000 for proofshare nodes with steady workload.
max_batch_wait_ms = 10000

# Reorder NORMAL-priority queue to group by circuit type.
# This reduces SRS context switches on GPUs with limited VRAM.
sort_by_type = true

[synthesis]
# CPU threads for circuit synthesis (rayon global pool). 0 = auto (all CPUs).
# Controls the rayon thread pool used by bellperson synthesis and PCE SpMV.
#
# When using parallel synthesis (synthesis_concurrency > 1), partition
# CPU cores between synthesis and the GPU thread pool:
#
# Example for 96-core machine:
#   threads = 64        → synthesis gets 64 rayon threads
#   [gpus] gpu_threads = 32  → GPU b_g2_msm gets 32 threads
#   [pipeline] synthesis_concurrency = 2  → 2 synthesis tasks share 64 threads
#
# With sequential synthesis (synthesis_concurrency=1), leave at 0 to let
# both pools use all CPUs (no contention since they don't overlap).
threads = 0

# Number of concurrent partition synthesis workers (Phase 7).
#
# Each worker processes one PoRep partition at a time (~29s each, mostly
# single-threaded witness generation). With 20 workers and 10 partitions
# per sector, 2 sectors can be synthesized simultaneously, keeping the
# GPU continuously fed with zero idle gaps between sectors.
#
# The workers feed individual partitions to the GPU channel. Each GPU call
# uses num_circuits=1, making b_g2_msm 0.4s instead of 25s (62x speedup
# on this step alone).
#
# Memory formula: Peak RSS ≈ 69 + (pw × 20) GiB
#   (69 GiB baseline = SRS 44 GiB + PCE 26 GiB)
#
# Recommended configs by system RAM (measured, gt=32):
#
#   RAM       pw   gw   Peak RSS   Throughput    Notes
#   -------   --   --   --------   ----------    -----
#   128 GiB    2    1    110 GiB   152 s/proof   Minimum viable
#   256 GiB    7    1    208 GiB    53 s/proof   Best value
#   384 GiB   10    2    271 GiB    43 s/proof   Near-optimal
#   512 GiB   12    2    373 GiB    43 s/proof   Same throughput, more headroom
#   768 GiB   12    2    400 GiB    38 s/proof   Full pipeline (j=20)
#
# At pw <= 7, gw=2 adds no benefit — set gw=1 instead.
# At pw >= 10, gw=2 gives ~6% throughput gain.
# pw > 12 uses more RAM with no throughput gain.
# pw > 16 regresses due to DDR5 bandwidth saturation.
#
# 0 = disabled — falls back to batch-all or Phase 6 slotted pipeline.
partition_workers = 12

[pipeline]
# Enable pipelined synthesis → GPU proving (Phase 2).
# When enabled, a dedicated synthesis task pre-synthesizes proofs on CPU
# and feeds them to GPU workers via a bounded channel. This allows CPU
# synthesis of proof N+1 to overlap with GPU proving of proof N:
#
#   Synthesis:  [=== proof N ===][=== proof N+1 ===][=== proof N+2 ===]
#   GPU:                         [=== proof N ===]  [=== proof N+1 ===]
#
# Steady-state throughput = max(synth_time, gpu_time) per proof.
# For PoRep 32G: ~55s/proof (synthesis-bound) vs ~91s sequential.
#
# When disabled, falls back to monolithic Phase 1 proving (no overlap).
enabled = true

# Max pre-synthesized proofs buffered between synthesis and GPU stages.
# Controls memory backpressure — each in-flight PoRep 32G proof is ~136 GiB
# of intermediate state (10 partitions × ~13.6 GiB each).
#   1 = one proof pre-synthesized (recommended for PoRep on 512+ GiB machines)
#   2+ = more aggressive lookahead (only for PoSt which has small state)
# The synthesis task blocks when the channel is full, preventing OOM.
synthesis_lookahead = 1

# Number of concurrent CPU synthesis tasks.
#
# When synthesis takes longer than GPU proving (e.g. 39s synth vs 27s GPU),
# the GPU idles for ~12s between proofs with a single synthesis task.
# With 2 concurrent synthesis tasks, the GPU can be kept fully saturated.
#
#   1 = sequential synthesis (default, lower memory)
#   2 = recommended for single-GPU machines with >=400 GiB RAM
#   N = N concurrent syntheses (each PoRep 32G synth uses ~136 GiB)
#
# Setting this > 1 subsumes the benefit of cross-sector batching
# (max_batch_size > 1) for throughput, while using the same total memory.
synthesis_concurrency = 1

[logging]
# Log level: trace, debug, info, warn, error
# Can also be overridden via RUST_LOG env var.
level = "info"

# Log format: unset for human-readable, "json" for structured.
# format = "json"
