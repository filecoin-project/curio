# Configuration for supra_seal
spdk: {
  # PCIe identifiers of NVMe drives to use to store layers
  nvme = [ "0000:01:00.0",
           "0000:02:00.0",
           "0000:03:00.0",
           "0000:29:00.0",
           "0000:2a:00.0",
           "0000:2b:00.0",
           "0000:2c:00.0",
           "0000:41:00.0",
           "0000:44:00.0",
           "0000:62:00.0",
           "0000:63:00.0",
           "0000:64:00.0",
           "0000:65:00.0" ];
}

# CPU topology for various parallel sector counts
topology:
{
  pc1: {
    # Core for writing hashed nodes to disk
    writer       = 0;
    # Core for reading parent nodes
    reader       = 1;
    # Core for coordinating buffers
    orchestrator = 2;

    # Number of hashers to instantiate per physical core
    hashers_per_core = 1;

    # Configuration for coordinators and hashers for various parallel sector counts.
    # Each entry has the following fields:
    #   sectors - the number of parallel sectors supported
    #   coordinators - a list of one or more coordinator nodes, each containing
    #     core - which core the coordinator runs on
    #     hashers - the number of hashing threads associated with the coordinator
    # Each hashing thread processes two sectors. As a result the sum of "hashers"
    # times two should equal "sectors".
    # 
    # It's important to take into account the topology of the system when arranging
    # threads. The purpose of the coordinator is to load data into the L3 cache so
    # that the associated hashing threads have low latency access to the data. For
    # this to be effective they must share the L3 cache. The system topology can be
    # conveniently visualized using the `lstopo` command.
    # 
    # The typical configuration would be one coordinator per core complex (CCX) on
    # an AMD based machine to maximize cache data locality between the coordinator and
    # hashing threads. 
    #
    # To illustrate consider the configuration for 64 parallel sectors. Cores 0, 1 and 2
    # are used by the writer, reader, and orchestrator threads, so the first coordinator
    # is placed on core 3. There are 4 more physical cores availabe in the CCX, so 8
    # hashers are assigned to utilize both the physical and hyperthread cores. This covers
    # the first 16 sectors (2 sectors per hasher).
    #
    # The next coordinator is assigned to core 8. There are then 7 physical cores
    # remaining, so 14 hashing threads are assigned, bring the sector count to 44.
    # Finally core 16 gets the last coordinator with 10 hashers to cover the remaining
    # 20 sectors. 
    sector_configs: (
      {
        sectors = 2;
        coordinators = (
          { core = 3;
            hashers = 1; }
        )
      },
      {
        sectors = 4;
        coordinators = (
          { core = 3;
            hashers = 2; }
        )
      },
      {
        sectors = 8;
        coordinators = (
          { core = 3;
            hashers = 4; }
        )
      },
      {
        sectors = 16;
        coordinators = (
          { core = 3;
            hashers = 4; },
          { core = 8;
            hashers = 4; }
        )
      },
      {
        sectors = 32;
        coordinators = (
          { core = 3;
            hashers = 4; },
          { core = 8;
            hashers = 7; },
          { core = 16;
            hashers = 5; }
        )
      }
    )
  },
  # TODO: This conflicts with 128 sectors, but our current processor runs
  # out of cores. 
  pc2: {
    # Core for reading columns from NVMe
    reader = 24;
    # Core for initiating layer reading and managing Poseidon hashing on GPU(s)
    hasher = 25;
    # Core for writing hashed data to tree-r and tree-c files
    writer = 26;
  },
  c1: {
    # Core for reading nodes from NVMe
    reader = 27;
  }
}